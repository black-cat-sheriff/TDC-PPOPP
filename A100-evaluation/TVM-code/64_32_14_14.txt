ConfigSpace (len=21676032, space_map=
   0 tile_f: Split(policy=factors, product=32, num_outputs=4) len=56
   1 tile_y: Split(policy=factors, product=14, num_outputs=4) len=16
   2 tile_x: Split(policy=factors, product=14, num_outputs=4) len=16
   3 tile_rc: Split(policy=factors, product=64, num_outputs=3) len=28
   4 tile_ry: Split(policy=factors, product=3, num_outputs=3) len=3
   5 tile_rx: Split(policy=factors, product=3, num_outputs=3) len=3
   6 auto_unroll_max_step: OtherOption([0, 512, 1500]) len=3
   7 unroll_explicit: OtherOption([0, 1]) len=2
)
Finish loading 150 records

Best config:
[('tile_f', [-1, 1, 32, 1]), ('tile_y', [-1, 1, 1, 1]), ('tile_x', [-1, 1, 2, 1]), ('tile_rc', [-1, 4, 4]), ('tile_ry', [-1, 1, 3]), ('tile_rx', [-1, 3, 1]), ('auto_unroll_max_step', 512), ('unroll_explicit', 0)],None,5838356
Finish loading 150 records
/uufs/chpc.utah.edu/common/home/u0814474/soft/tvm-python/tvm/python/tvm/target/target.py:454: UserWarning: tvm.target.create() is being deprecated. Please use tvm.target.Target() instead
  warnings.warn("tvm.target.create() is being deprecated. Please use tvm.target.Target() instead")
Dump


******6*******

extern "C" __global__ void default_function_kernel0(float* __restrict__ data, float* __restrict__ kernel, float* __restrict__ compute) {
  float compute_local[1];
  __shared__ float pad_temp_shared[192];
  __shared__ float kernel_shared[4608];
  float pad_temp_shared_local[12];
  float kernel_shared_local[12];
  compute_local[(0)] = 0.000000e+00f;
  for (int rc_outer = 0; rc_outer < 4; ++rc_outer) {
    __syncthreads();
    #pragma unroll
    for (int ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner = 0; ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner < 3; ++ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) {
      pad_temp_shared[((((((int)threadIdx.z) * 6) + (((int)threadIdx.x) * 3)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner))] = (((((1 <= ((((((((int)threadIdx.z) * 6) + (((int)threadIdx.x) * 3)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) % 12) >> 2) + ((int)blockIdx.y))) && (((((((((int)threadIdx.z) * 6) + (((int)threadIdx.x) * 3)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) % 12) >> 2) + ((int)blockIdx.y)) < 15)) && (1 <= ((((int)blockIdx.x) * 2) + ((((((int)threadIdx.z) * 6) + (((int)threadIdx.x) * 3)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) & 3)))) && (((((int)blockIdx.x) * 2) + ((((((int)threadIdx.z) * 6) + (((int)threadIdx.x) * 3)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) & 3)) < 15)) ? data[((((((((rc_outer * 3136) + (((((((int)threadIdx.z) * 6) + (((int)threadIdx.x) * 3)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) / 12) * 196)) + ((((((((int)threadIdx.z) * 6) + (((int)threadIdx.x) * 3)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) % 12) >> 2) * 14)) + (((int)blockIdx.y) * 14)) + (((int)blockIdx.x) * 2)) + ((((((int)threadIdx.z) * 6) + (((int)threadIdx.x) * 3)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) & 3)) - 15))] : 0.000000e+00f);
    }
    #pragma unroll
    for (int ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1 < 72; ++ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1) {
      kernel_shared[((((((int)threadIdx.z) * 144) + (((int)threadIdx.x) * 72)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1))] = kernel[(((((((int)threadIdx.z) * 576) + (rc_outer * 144)) + (((int)threadIdx.x) * 72)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1))];
    }
    __syncthreads();
    #pragma unroll
    for (int rc_inner_outer = 0; rc_inner_outer < 4; ++rc_inner_outer) {
      #pragma unroll
      for (int ry_inner_outer = 0; ry_inner_outer < 3; ++ry_inner_outer) {
        #pragma unroll
        for (int ax1 = 0; ax1 < 4; ++ax1) {
          #pragma unroll
          for (int ax3 = 0; ax3 < 3; ++ax3) {
            pad_temp_shared_local[(((ax1 * 3) + ax3))] = pad_temp_shared[((((((rc_inner_outer * 48) + (ax1 * 12)) + (ry_inner_outer * 4)) + ax3) + ((int)threadIdx.x)))];
          }
        }
        #pragma unroll
        for (int ax11 = 0; ax11 < 4; ++ax11) {
          #pragma unroll
          for (int ax31 = 0; ax31 < 3; ++ax31) {
            kernel_shared_local[(((ax11 * 3) + ax31))] = kernel_shared[((((((((int)threadIdx.z) * 144) + (rc_inner_outer * 36)) + (ax11 * 9)) + (ry_inner_outer * 3)) + ax31))];
          }
        }
        #pragma unroll
        for (int rc_inner_inner = 0; rc_inner_inner < 4; ++rc_inner_inner) {
          #pragma unroll
          for (int rx_inner_inner = 0; rx_inner_inner < 3; ++rx_inner_inner) {
            compute_local[(0)] = (compute_local[(0)] + (pad_temp_shared_local[(((rc_inner_inner * 3) + rx_inner_inner))] * kernel_shared_local[(((rc_inner_inner * 3) + rx_inner_inner))]));
          }
        }
      }
    }
  }
  compute[(((((((int)threadIdx.z) * 196) + (((int)blockIdx.y) * 14)) + (((int)blockIdx.x) * 2)) + ((int)threadIdx.x)))] = compute_local[(0)];
}



******6*******

Time cost of this operator: 0.000031
cnm
[src/runtime/cuda/cuda_module.cc] 
 grid=(7,14,1),  block=(2,1,32)
[src/runtime/cuda/cuda_module.cc] 
 grid=(7,14,1),  block=(2,1,32)
[src/runtime/cuda/cuda_module.cc] 
 grid=(7,14,1),  block=(2,1,32)
