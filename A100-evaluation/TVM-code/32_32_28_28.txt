ConfigSpace (len=101606400, space_map=
   0 tile_f: Split(policy=factors, product=32, num_outputs=4) len=56
   1 tile_y: Split(policy=factors, product=28, num_outputs=4) len=40
   2 tile_x: Split(policy=factors, product=28, num_outputs=4) len=40
   3 tile_rc: Split(policy=factors, product=32, num_outputs=3) len=21
   4 tile_ry: Split(policy=factors, product=3, num_outputs=3) len=3
   5 tile_rx: Split(policy=factors, product=3, num_outputs=3) len=3
   6 auto_unroll_max_step: OtherOption([0, 512, 1500]) len=3
   7 unroll_explicit: OtherOption([0, 1]) len=2
)
Finish loading 150 records

Best config:
[('tile_f', [-1, 1, 4, 4]), ('tile_y', [-1, 1, 4, 1]), ('tile_x', [-1, 1, 4, 1]), ('tile_rc', [-1, 2, 4]), ('tile_ry', [-1, 1, 1]), ('tile_rx', [-1, 3, 1]), ('auto_unroll_max_step', 1500), ('unroll_explicit', 0)],None,40611803
Finish loading 150 records
/uufs/chpc.utah.edu/common/home/u0814474/soft/tvm-python/tvm/python/tvm/target/target.py:454: UserWarning: tvm.target.create() is being deprecated. Please use tvm.target.Target() instead
  warnings.warn("tvm.target.create() is being deprecated. Please use tvm.target.Target() instead")
Dump


******6*******

extern "C" __global__ void default_function_kernel0(float* __restrict__ data, float* __restrict__ kernel, float* __restrict__ compute) {
  float compute_local[4];
  __shared__ float pad_temp_shared[192];
  __shared__ float kernel_shared[384];
  float pad_temp_shared_local[4];
  float kernel_shared_local[16];
  #pragma unroll
  for (int ff_c_init = 0; ff_c_init < 4; ++ff_c_init) {
    compute_local[(ff_c_init)] = 0.000000e+00f;
  }
  for (int rc_outer = 0; rc_outer < 4; ++rc_outer) {
    #pragma unroll
    for (int rx_outer = 0; rx_outer < 3; ++rx_outer) {
      __syncthreads();
      #pragma unroll
      for (int ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner = 0; ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner < 3; ++ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) {
        pad_temp_shared[(((((((int)threadIdx.z) * 48) + (((int)threadIdx.y) * 12)) + (((int)threadIdx.x) * 3)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner))] = (((((1 <= ((((int)blockIdx.y) * 4) + (((((int)threadIdx.y) * 3) + (((((int)threadIdx.x) * 3) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) >> 2)) % 6))) && (((((int)blockIdx.y) * 4) + (((((int)threadIdx.y) * 3) + (((((int)threadIdx.x) * 3) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) >> 2)) % 6)) < 29)) && (1 <= (((((int)blockIdx.x) * 4) + rx_outer) + (((((int)threadIdx.x) * 3) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) & 3)))) && ((((((int)blockIdx.x) * 4) + rx_outer) + (((((int)threadIdx.x) * 3) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) & 3)) < 29)) ? data[((((((((((rc_outer * 6272) + (((int)threadIdx.z) * 1568)) + ((((((int)threadIdx.y) * 3) + (((((int)threadIdx.x) * 3) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) >> 2)) / 6) * 784)) + (((int)blockIdx.y) * 112)) + ((((((int)threadIdx.y) * 3) + (((((int)threadIdx.x) * 3) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) >> 2)) % 6) * 28)) + (((int)blockIdx.x) * 4)) + rx_outer) + (((((int)threadIdx.x) * 3) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) & 3)) - 29))] : 0.000000e+00f);
      }
      #pragma unroll
      for (int ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1 < 6; ++ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1) {
        kernel_shared[(((((((int)threadIdx.z) * 96) + (((int)threadIdx.y) * 24)) + (((int)threadIdx.x) * 6)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1))] = kernel[((((((((((int)blockIdx.z) * 4608) + (((int)threadIdx.z) * 1152)) + (((int)threadIdx.y) * 288)) + (rc_outer * 72)) + (((int)threadIdx.x) * 18)) + (ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1 * 3)) + rx_outer))];
      }
      __syncthreads();
      #pragma unroll
      for (int rc_inner_outer = 0; rc_inner_outer < 2; ++rc_inner_outer) {
        #pragma unroll
        for (int ry_inner_outer = 0; ry_inner_outer < 3; ++ry_inner_outer) {
          #pragma unroll
          for (int ax1 = 0; ax1 < 4; ++ax1) {
            pad_temp_shared_local[(ax1)] = pad_temp_shared[((((((rc_inner_outer * 96) + (ax1 * 24)) + (((int)threadIdx.y) * 4)) + (ry_inner_outer * 4)) + ((int)threadIdx.x)))];
          }
          #pragma unroll
          for (int ax0 = 0; ax0 < 4; ++ax0) {
            #pragma unroll
            for (int ax11 = 0; ax11 < 4; ++ax11) {
              kernel_shared_local[(((ax0 * 4) + ax11))] = kernel_shared[((((((((int)threadIdx.z) * 96) + (ax0 * 24)) + (rc_inner_outer * 12)) + (ax11 * 3)) + ry_inner_outer))];
            }
          }
          #pragma unroll
          for (int rc_inner_inner = 0; rc_inner_inner < 4; ++rc_inner_inner) {
            #pragma unroll
            for (int ff_c = 0; ff_c < 4; ++ff_c) {
              compute_local[(ff_c)] = (compute_local[(ff_c)] + (pad_temp_shared_local[(rc_inner_inner)] * kernel_shared_local[(((ff_c * 4) + rc_inner_inner))]));
            }
          }
        }
      }
    }
  }
  #pragma unroll
  for (int ff_inner_inner_inner = 0; ff_inner_inner_inner < 4; ++ff_inner_inner_inner) {
    compute[((((((((((int)blockIdx.z) * 12544) + (((int)threadIdx.z) * 3136)) + (ff_inner_inner_inner * 784)) + (((int)blockIdx.y) * 112)) + (((int)threadIdx.y) * 28)) + (((int)blockIdx.x) * 4)) + ((int)threadIdx.x)))] = compute_local[(ff_inner_inner_inner)];
  }
}



******6*******

Time cost of this operator: 0.000019
cnm
[src/runtime/cuda/cuda_module.cc] 
 grid=(7,7,2),  block=(4,4,4)
[src/runtime/cuda/cuda_module.cc] 
 grid=(7,7,2),  block=(4,4,4)
[src/runtime/cuda/cuda_module.cc] 
 grid=(7,7,2),  block=(4,4,4)
